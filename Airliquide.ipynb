{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHROzvb6RR_B"
   },
   "source": [
    "Let's setup Spark on your Colab environment. Run the cell below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JMAP97VRPJc",
    "outputId": "e208695d-ef25-4245-946e-6437cb03a330",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (from pyspark) (0.10.9)\n",
      "Requirement already satisfied: keras in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (from keras) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (from keras) (1.6.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: scikit-surprise in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (from scikit-surprise) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.11.2 in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (from scikit-surprise) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (from scikit-surprise) (1.6.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (from scikit-surprise) (1.15.0)\n",
      "Requirement already satisfied: findspark in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: wget in c:\\users\\ngani\\anaconda3\\envs\\mlenv\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install -U -q PyDrive\n",
    "!pip install keras\n",
    "!pip install scikit-surprise\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "!pip install findspark\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "HPRcHRTOOYGM",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkConf, SparkContext, sql\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import findspark\n",
    "findspark.add_packages('mysql:mysql-connector-java:8.0.11')\n",
    "import keras\n",
    "import os.path\n",
    "from os import path\n",
    "from zipfile import ZipFile\n",
    "import wget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6I5Nx1YW2NG"
   },
   "source": [
    "# **1. Download the movie lens dataset and extract** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVWN2wl0Rl2X",
    "outputId": "2a9686a2-826e-411e-b1fb-60d1231f2e15",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset already exist. No need to download \n"
     ]
    }
   ],
   "source": [
    "# Download the actual data from http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
    "dataset_path = \"dataset/\"\n",
    "\n",
    "if not path.exists(dataset_path):\n",
    "    !mkdir dataset\n",
    "    #!wget -P dataset/ https://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
    "    data_url = \"https://files.grouplens.org/datasets/movielens/ml-25m.zip\"\n",
    "    data_url1 = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
    "    wget.download(data_url1, 'dataset/')\n",
    "else:\n",
    "  print(\"dataset already exist. No need to download \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XM0Urcd-XqNl",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lbbTsBYLXqsM",
    "outputId": "d607d157-48b5-4119-bc46-dbc36eb83eab",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all the files now...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Only extract the data the first time the script is run.\n",
    "movielens_dir = dataset_path + \"/ml-25m\"\n",
    "movielens_zipped_file = dataset_path + \"ml-25m.zip\"\n",
    "\n",
    "if not path.exists(movielens_dir):\n",
    "    with ZipFile(movielens_zipped_file, \"r\") as zip:\n",
    "        # Extract files\n",
    "        print(\"Extracting all the files now...\")\n",
    "        zip.extractall(path=dataset_path)\n",
    "        print(\"Done!\")\n",
    "else:\n",
    "   print(\"dataset already exist. No need to extract \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_dvBEIbfwad"
   },
   "source": [
    "# **2. Setup the big data environment with pyspark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QeO3m6r2fI_Q",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-28-7aa24d27afdd>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mconf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"spark.ui.port\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"4050\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# create the context\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0msc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mspark\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSparkSession\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pyspark\\context.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[0;32m    142\u001B[0m                 \" is not allowed as it is a security risk.\")\n\u001B[0;32m    143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 144\u001B[1;33m         \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    145\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    146\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001B[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pyspark\\context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[1;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[0;32m    329\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    330\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_gateway\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 331\u001B[1;33m                 \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_gateway\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgateway\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mlaunch_gateway\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    332\u001B[0m                 \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_gateway\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjvm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pyspark\\java_gateway.py\u001B[0m in \u001B[0;36mlaunch_gateway\u001B[1;34m(conf, popen_kwargs)\u001B[0m\n\u001B[0;32m    103\u001B[0m             \u001B[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    104\u001B[0m             \u001B[1;32mwhile\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mproc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpoll\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn_info_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 105\u001B[1;33m                 \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    106\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn_info_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# create the session\n",
    "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
    "# create the context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Movie recommendation\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "NOnHeue9gIax",
    "outputId": "4e01c003-8f0f-42f0-eb14-6f4ba45fa9e7",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFViQ0YEgODU"
   },
   "source": [
    "# **3. Data exploration and cleaning**\n",
    "\n",
    "\n",
    "**a .Reading the downloaded movie dataset to pyspark dataframe**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_file = movielens_dir + \"/ratings.csv\"\n",
    "movies_file = movielens_dir + \"/movies.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSAwBblbgLFO",
    "outputId": "ac1b9fae-2759-42e8-d17a-f8c25c472589",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the dataset schema \n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "ratings_df_schema = StructType(\n",
    "  [StructField('userId', IntegerType()),\n",
    "   StructField('movieId', IntegerType()),\n",
    "   StructField('rating', DoubleType())]\n",
    ")\n",
    "movies_df_schema = StructType(\n",
    "  [StructField('movieId', IntegerType()),\n",
    "   StructField('title', StringType()),\n",
    "   StructField('genres', StringType())]\n",
    ")\n",
    "\n",
    "# creating the pyspark dataframes and cache in memory\n",
    "\n",
    "ratings_df = spark.read\\\n",
    "                  .options(header =True, inferSchema=False)\\\n",
    "                  .schema(ratings_df_schema)\\\n",
    "                  .csv(ratings_file)\n",
    "movies_df = spark.read\\\n",
    "                .options(header =True, inferSchema=False)\\\n",
    "                .schema(movies_df_schema)\\\n",
    "                .csv(movies_file)\n",
    "\n",
    "ratings_df.cache()\n",
    "movies_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oAFYqzShHuj",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ratings_df.show(10, truncate=False)\n",
    "movies_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KI_9mY4sm-xk",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ratings_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjGozDwbm8J2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "movies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSuO9fgB_Y2p"
   },
   "source": [
    "# **4. Building the user-based collaborative filtering**\n",
    "\n",
    "\n",
    "\n",
    "> For the this project, we will use the SurPRISE (Simple Python RecommendatIon System Engine) library\n",
    "\n",
    "\n",
    "> This is because it is faster and has an integrated SVD algorithms, a Matrix factorization algoritms\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "3gaKcH42qE2g",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries from Surprise package\n",
    "\n",
    "from surprise import Reader, Dataset, SVD, SVDpp\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fWISV0w-EnFQ",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Surprise is only compatible with pandas. So we will convert the pyspark dataframes to pandas dataframes\n",
    "#ratings_df_pd =ratings_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6-MdkDSPEnPD",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ratings_pd = pd.read_csv(ratings_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_pd = ratings_pd[['userId', 'movieId', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "kkDyoI3YGY0l",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000090</th>\n",
       "      <td>162541</td>\n",
       "      <td>50872</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000091</th>\n",
       "      <td>162541</td>\n",
       "      <td>55768</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000092</th>\n",
       "      <td>162541</td>\n",
       "      <td>56176</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000093</th>\n",
       "      <td>162541</td>\n",
       "      <td>58559</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000094</th>\n",
       "      <td>162541</td>\n",
       "      <td>63876</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000095 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  rating\n",
       "0              1      296     5.0\n",
       "1              1      306     3.5\n",
       "2              1      307     5.0\n",
       "3              1      665     5.0\n",
       "4              1      899     3.5\n",
       "...          ...      ...     ...\n",
       "25000090  162541    50872     4.5\n",
       "25000091  162541    55768     2.5\n",
       "25000092  162541    56176     2.0\n",
       "25000093  162541    58559     4.0\n",
       "25000094  162541    63876     5.0\n",
       "\n",
       "[25000095 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0f_GymXyGaqM",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId       int64\n",
       "movieId      int64\n",
       "rating     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_pd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHRaqc-PGas8",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "aSXf6o97EaGK",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mEmpty\u001B[0m                                     Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001B[0m in \u001B[0;36mdispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    819\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 820\u001B[1;33m                 \u001B[0mtasks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_ready_batches\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mblock\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    821\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mqueue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmpty\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\mlenv\\lib\\queue.py\u001B[0m in \u001B[0;36mget\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    166\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_qsize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 167\u001B[1;33m                     \u001B[1;32mraise\u001B[0m \u001B[0mEmpty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    168\u001B[0m             \u001B[1;32melif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mEmpty\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-38-3cb329fac508>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;31m# Run 5-fold cross-validation and print results.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m \u001B[0mcross_validate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msvd\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmeasures\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'RMSE'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'MAE'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\surprise\\model_selection\\validation.py\u001B[0m in \u001B[0;36mcross_validate\u001B[1;34m(algo, data, measures, cv, return_train_measures, n_jobs, pre_dispatch, verbose)\u001B[0m\n\u001B[0;32m    101\u001B[0m                                            return_train_measures)\n\u001B[0;32m    102\u001B[0m                     for (trainset, testset) in cv.split(data))\n\u001B[1;32m--> 103\u001B[1;33m     \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mParallel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_jobs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mn_jobs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpre_dispatch\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpre_dispatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdelayed_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    104\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    105\u001B[0m     (test_measures_dicts,\n",
      "\u001B[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1039\u001B[0m             \u001B[1;31m# remaining jobs.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1040\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_iterating\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1041\u001B[1;33m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdispatch_one_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1042\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_iterating\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_original_iterator\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1043\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001B[0m in \u001B[0;36mdispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    829\u001B[0m                 \u001B[0mbig_batch_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbatch_size\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    830\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 831\u001B[1;33m                 \u001B[0mislice\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mitertools\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mislice\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbig_batch_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    832\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mislice\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    833\u001B[0m                     \u001B[1;32mreturn\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\surprise\\model_selection\\validation.py\u001B[0m in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     98\u001B[0m     \u001B[0mcv\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_cv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     99\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 100\u001B[1;33m     delayed_list = (delayed(fit_and_score)(algo, trainset, testset, measures,\n\u001B[0m\u001B[0;32m    101\u001B[0m                                            return_train_measures)\n\u001B[0;32m    102\u001B[0m                     for (trainset, testset) in cv.split(data))\n",
      "\u001B[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\surprise\\model_selection\\split.py\u001B[0m in \u001B[0;36msplit\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    115\u001B[0m             \u001B[0mraw_testset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraw_ratings\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mindices\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mstop\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 117\u001B[1;33m             \u001B[0mtrainset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconstruct_trainset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mraw_trainset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m             \u001B[0mtestset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconstruct_testset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mraw_testset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\surprise\\dataset.py\u001B[0m in \u001B[0;36mconstruct_trainset\u001B[1;34m(self, raw_trainset)\u001B[0m\n\u001B[0;32m    186\u001B[0m         \u001B[1;31m# user raw id, item raw id, translated rating, time stamp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    187\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0murid\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mirid\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimestamp\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mraw_trainset\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 188\u001B[1;33m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    189\u001B[0m                 \u001B[0muid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mraw2inner_id_users\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0murid\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    190\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Load Reader library\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Load ratings dataset with Dataset library\n",
    "data = Dataset.load_from_df(ratings_pd, reader)\n",
    "\n",
    "\n",
    "svd = SVD(n_factors=50)\n",
    "#svd_plusplus = SVDpp(n_factors=50)\n",
    "\n",
    "\n",
    "# Run 5-fold cross-validation and print results.\n",
    "\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4tc8M6kGsJg",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Build the the training set and fit the model\n",
    "\n",
    "trainset = dataset.build_full_trainset()\n",
    "\n",
    "svd.fit(trainset)  # old version use svd.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2ajZeL3GtSu",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Airliquide.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyCharm (philippe_test)",
   "language": "python",
   "name": "pycharm-ada88a1a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}